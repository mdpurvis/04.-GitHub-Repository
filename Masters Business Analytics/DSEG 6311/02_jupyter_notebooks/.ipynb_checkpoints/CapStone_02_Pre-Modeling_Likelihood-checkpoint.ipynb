{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Data Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# EDA - Data Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "import statsmodels.api as stats\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('axes', titlesize=18)\n",
    "\n",
    "#Modeling\n",
    "from sklearn import (datasets,\n",
    "                     metrics,\n",
    "                     model_selection as skms,\n",
    "                     naive_bayes,\n",
    "                     neighbors)\n",
    "\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                 SGDClassifier)\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     cross_val_predict,\n",
    "                                     train_test_split,\n",
    "                                     GridSearchCV)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             confusion_matrix,\n",
    "                             f1_score,\n",
    "                             roc_curve,\n",
    "                             auc,\n",
    "                             classification_report,\n",
    "                             precision_recall_curve)\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              AdaBoostClassifier,\n",
    "                              ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifierCV \n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# logistic regression model\n",
    "import statsmodels.api as sm \n",
    "\n",
    "pd.set_option('display.max_rows', 90)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_summary(X_train, y_train):\n",
    "\n",
    "    print('\\nTraining Dataset - Before Synthetic Minority Oversampling Technique (SMOTE): ')\n",
    "    print('   Number  of Companies sued:               {:6,.0f}'.format(sum(y_train['suitflag'])))\n",
    "    print('   Number of Companies not sued:            {:6,.0f}'.format(len(y_train)-sum(y_train['suitflag'])))\n",
    "    print('   Total Companies:                         {:6,.0f}'.format(len(y_train)))\n",
    "    print('   Percent of Companies who Filed Suit:    {:6,.0f}%'.format(sum(y_train['suitflag'])/len(y_train)*100))\n",
    "    print('   Columns and Row count Review: ')\n",
    "    print('      Column count:                         {:6,.0f}'.format(len(x.columns)))\n",
    "    print('      Column-to-Row ratio:                 {:6,.0f}%'.format(len(x.columns)/len(y_train)*100)+'\\n')\n",
    "    \n",
    "    print('Training Dataset - After SMOTE: ')\n",
    "    print('   Training Dataset - Overview: ')\n",
    "    print('   Number  of Companies sued (Increase):    {:6,.0f}'.format(sum(y_train_smote['suitflag'])))\n",
    "    print('   Number of Companies not sued:            {:6,.0f}'.format(len(y_train_smote)-sum(y_train_smote['suitflag'])))\n",
    "    print('   Percent of Companies who Filed Suit:    {:6,.0f}%'.format(sum(y_train_smote['suitflag'])/len(y_train_smote)*100))\n",
    "    print('   Columns and Row count Review: ')\n",
    "    print('      Column count:                         {:6,.0f}'.format(len(x.columns)))\n",
    "    print('      Column-to-Row ratio:                 {:6,.0f}%'.format(len(x.columns)/len(y_train)*100)+'\\n')\n",
    "    \n",
    "    print('Test Dataset - Overview: ')\n",
    "    print('   Number  of Companies sued:               {:6,.0f}'.format(sum(y_test['suitflag'])))\n",
    "    print('   Number of Companies not sued:            {:6,.0f}'.format(len(y_test)-sum(y_test['suitflag'])))\n",
    "    print('   Total Companies:                         {:6,.0f}'.format(len(y_test)))\n",
    "    print('   Percent of Companies who Filed Suit:    {:6,.0f}%'.format(sum(y_test['suitflag'])/len(y_test)*100))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def model_run(X_train, y_train, model_run, results_review):\n",
    "    if results_review == 'train':    \n",
    "        \n",
    "        accuracy = round(model_run.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "        prec_score = round(precision_score(y_true = y_train, \n",
    "                                           y_pred = model_pred_train)* 100, 2)\n",
    "\n",
    "        recall = round(recall_score(y_true = y_train, \n",
    "                                    y_pred = model_pred_train)* 100, 2)\n",
    "\n",
    "        f1 = round(f1_score(y_true = y_train, \n",
    "                            y_pred = model_pred_train)* 100, 2)\n",
    "\n",
    "        print('Summary of Modeled Results: ')\n",
    "        print('   General Accuracy: {:6,.1f}%'.format(accuracy))\n",
    "        print('   ROC AUC Score:    {:6,.1f}%'.format(metrics.roc_auc_score(y_train, model_pred_train)*100))\n",
    "        print('   Precision Score:  {:6,.1f}%'.format(prec_score))\n",
    "        print('   Recall Score:     {:6,.1f}%'.format(recall))\n",
    "        print('   F1 Score:         {:6,.1f}%'.format(f1)+'\\n')\n",
    "\n",
    "\n",
    "        # The confusion matrix\n",
    "        sns.set(font_scale = 1.5)\n",
    "        cm = confusion_matrix(y_train, model_run.predict(X_train))\n",
    "        f, ax = plt.subplots(figsize=(5,5))\n",
    "        sns.heatmap(cm, \n",
    "                    annot=True, \n",
    "                    linewidth=0.7, \n",
    "                    linecolor='black', \n",
    "                    fmt='g', \n",
    "                    ax=ax, \n",
    "                    cmap=\"BuPu\")\n",
    "        plt.xlabel('Suit Prediction')\n",
    "        plt.ylabel('Suit Actual')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        \n",
    "        accuracy = round(model_run.score(X_test, y_test) * 100, 2)\n",
    "\n",
    "        prec_score = round(precision_score(y_true = y_test, \n",
    "                                           y_pred = model_pred_test)* 100, 2)\n",
    "\n",
    "        recall = round(recall_score(y_true = y_test, \n",
    "                                    y_pred = model_pred_test)* 100, 2)\n",
    "\n",
    "        f1 = round(f1_score(y_true = y_test, \n",
    "                            y_pred = model_pred_test)* 100, 2)\n",
    "\n",
    "        print('Summary of Modeled Results: ')\n",
    "        print('   General Accuracy: {:6,.1f}%'.format(accuracy))\n",
    "        print('   ROC AUC Score:    {:6,.1f}%'.format(metrics.roc_auc_score(y_train, model_pred_train)*100))\n",
    "        print('   Precision Score:  {:6,.1f}%'.format(prec_score))\n",
    "        print('   Recall Score:     {:6,.1f}%'.format(recall))\n",
    "        print('   F1 Score:         {:6,.1f}%'.format(f1)+'\\n')\n",
    "\n",
    "\n",
    "        # The confusion matrix\n",
    "        sns.set(font_scale = 1.5)\n",
    "        cm = confusion_matrix(y_test, model_run.predict(X_test))\n",
    "        f, ax = plt.subplots(figsize=(5,5))\n",
    "        sns.heatmap(cm, \n",
    "                    annot=True, \n",
    "                    linewidth=0.7, \n",
    "                    linecolor='black', \n",
    "                    fmt='g', \n",
    "                    ax=ax, \n",
    "                    cmap=\"BuPu\")\n",
    "        plt.xlabel('Suit Prediction')\n",
    "        plt.ylabel('Suit Actual')\n",
    "        plt.show()          \n",
    "\n",
    "    probs = model_run.predict_proba(X_test)\n",
    "    f, ax = plt.subplots(figsize=(5, 5))\n",
    "    # Calculate the fpr and tpr for all thresholds of the classification\n",
    "    fpr, tpr, threshold = roc_curve(y_test, probs[:,1])\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "        \n",
    "    return accuracy, prec_score, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_values(axs, orient=\"v\", space=.01):\n",
    "    def _single(ax):\n",
    "        if orient == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
    "                value = '{:.0f}'.format(p.get_height())\n",
    "                ax.text(_x, _y, value, ha=\"center\") \n",
    "        elif orient == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
    "                value = '{:.0f}'.format(p.get_width())\n",
    "                ax.text(_x, _y, value, ha=\"left\")\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _single(ax)\n",
    "    else:\n",
    "        _single(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../01_data/02_modified/capstone_modeling_final1.csv', index_col='gvkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Modeling Work - Final Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Dependant Variable Split (DV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot for the % of target variable\n",
    "sns.set(font_scale = 1.7)\n",
    "labels =df['suitflag'].value_counts(sort = True).index\n",
    "sizes = df['suitflag'].value_counts(sort = True)\n",
    "colors = [\"lightsteelblue\",\"salmon\"]\n",
    "explode = (0.1,0)  # explode 1st slice\n",
    " \n",
    "rcParams['figure.figsize'] = 8,8\n",
    "# Plot\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=270,)\n",
    "plt.title('Percent of Customers Who Have Had a Suit Filed Against Them - 0 = No; 1 = Yes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('capstone_modeling1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['restatementflag']\n",
    "\n",
    "for ea in range(len(cols)):\n",
    "    df[cols[ea]] = df[cols[ea]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols=[]\n",
    "multi_Value=[]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype =='object':\n",
    "        if df[col].unique().shape[0]==2:\n",
    "            binary_cols.append(col)\n",
    "        else:\n",
    "            multi_Value.append(col)\n",
    "            \n",
    "print('Multi-value columns include: ',multi_Value)\n",
    "print('Binary-value columns include: ',binary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.dtypes[df.dtypes == object].index:\n",
    "    print(col,'\\n', df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDBflag, with Foreign_and_Domestic_indicator (1 = yes; 0 = no)\n",
    "df['Foreign_and_Domestic_indicator'] = df['idbflag'].replace({'D':0, 'B':1})\n",
    "df.drop(columns='idbflag', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GIC_SubIndustry'] = df['GIC_SubIndustry'].replace({r' & ':'_', r' ':'_'}, regex=True)\n",
    "df['GIC_Industry'] = df['GIC_Industry'].replace({r' & ':'_', r' ':'_'}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIC_Industry = pd.get_dummies(df['GIC_Industry'], prefix='GIC_Industry').drop(columns=['GIC_Industry_Aeorspace & Defense'])\n",
    "# df1 = df.join(GIC_Industry)\n",
    "\n",
    "\n",
    "GIC_SubIndustry = pd.get_dummies(df['GIC_SubIndustry'], prefix='GIC_SI').drop(columns=['GIC_SI_Aerospace_Defense'])\n",
    "df1 = df.join(GIC_SubIndustry)\n",
    "\n",
    "# stko = pd.get_dummies(df['stko'], prefix='stko').drop(columns=['stko_0'])\n",
    "# df1 = df1.join(stko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns due to dummy variable additions\n",
    "df2 = df1.drop(columns=['GIC_Industry', \n",
    "                        'GIC_SubIndustry'\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['restatementflag'] = df2['restatementflag'].astype(int)\n",
    "df2['suitflag'] = df2['suitflag'].replace({'Yes':1, 'No':0})\n",
    "\n",
    "# drop restatment flag, as this was used in the prior steps when calculating restatement variances.\n",
    "df2.drop(columns='restatementflag', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output any correlated values over 0.67 - isolating high-correlated variables to remove from the analysis\n",
    "def high_corr_and_check(X):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), \n",
    "                                     k=1).astype(np.bool))\n",
    "                      .stack()\n",
    "                      .sort_values(ascending=False))\n",
    "    for index, value in sol.items():\n",
    "        if value > 0.65:\n",
    "            print(index,value)\n",
    "            \n",
    "high_corr_and_check(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Action: </b>Remove the highly correlated variables I believe would help the analysis and re-run the high_corr_and_check command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns=['StdDev_dp', 'StdDev_at', 'StdDev_cogs', 'Vol_xido_Variance', 'Vol_txt_Variance', \n",
    "                  'StdDev_teq', 'StdDev_txt', 'StdDev_capx', 'xsga_PercentChange', 'ppent_PercentChange', \n",
    "                  'cshfd_PercentChange', 'roa_PercentChange', 'Vol_xsga_Variance', 'Vol_ppent_Variance', \n",
    "                  'Vol_dltt_Variance', 'Vol_dp_Variance', 'Vol_xint_Variance', 'StdDev_sale', 'StdDev_xint', \n",
    "                  'StdDev_ppent', 'StdDev_xsga', 'Vol_cogs_Variance', 'StdDev_dltt', 'teq_PercentChange', \n",
    "                  'dltt_PercentChange', 'StdDev_ni'], \n",
    "         inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output any correlated values over 0.67 - isolating high-correlated variables to remove from the analysis\n",
    "def high_corr_and_check(X):\n",
    "    corr_matrix = X.corr().abs()\n",
    "    sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), \n",
    "                                     k=1).astype(np.bool))\n",
    "                      .stack()\n",
    "                      .sort_values(ascending=False))\n",
    "    for index, value in sol.items():\n",
    "        if value > 0.65:\n",
    "            print(index,value)\n",
    "            \n",
    "high_corr_and_check(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df2.corr()\n",
    "mask = np.zeros_like(corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]= True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "# sns.set(font_scale = .6)\n",
    "\n",
    "heatmap = sns.heatmap(corr_matrix,\n",
    "                      mask = mask,\n",
    "                      square = True,\n",
    "                      linewidths = .5,\n",
    "                      cmap = 'coolwarm',\n",
    "                      cbar_kws = {'shrink': .6,\n",
    "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
    "                      vmin = -1,\n",
    "                      vmax = 1,\n",
    "                      annot = True,\n",
    "                      annot_kws = {'size': 8})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add the column names as labels\n",
    "ax.set_yticklabels(corr_matrix.columns, rotation = 0)\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation = 45, horizontalalignment='right')\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})\n",
    "\n",
    "# plt.savefig('corr_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to ensure all values are numeric and convert all as necessary\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review the columns with null values in the dataset\n",
    "nulls = pd.DataFrame(df2.isnull().sum().sort_values(ascending = False), columns=['Amount'])\n",
    "nulls = nulls.loc[nulls['Amount'] > 0]\n",
    "# print('Shape of train dataset:', train.shape, '\\nMissing values for train dataset below:\\n', nulls)\n",
    "nulls = pd.DataFrame(nulls.loc[nulls['Amount'] > 0])\n",
    "# print('Shape of train dataset:', train.shape, '\\nMissing values for train dataset below:\\n', nulls)\n",
    "nulls.index.name='Columns With Missing Values'\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review the columns with null values in the dataset\n",
    "nulls = pd.DataFrame(df2.isnull().sum().sort_values(ascending = False), columns=['Amount'])\n",
    "nulls = nulls.loc[nulls['Amount'] > 0]\n",
    "# print('Shape of train dataset:', train.shape, '\\nMissing values for train dataset below:\\n', nulls)\n",
    "nulls = pd.DataFrame(nulls.loc[nulls['Amount'] > 0])\n",
    "# print('Shape of train dataset:', train.shape, '\\nMissing values for train dataset below:\\n', nulls)\n",
    "nulls.index.name='Columns With Missing Values'\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df2.drop(columns=['suitflag'])\n",
    "y = df2[['suitflag']] \n",
    "\n",
    "#split data by 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "#add SMOTE\n",
    "sm = SMOTE(random_state = 30)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x.index.isin(['3580'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_summary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(n_jobs=-1, random_state=15).fit(X_train, y_train)\n",
    "\n",
    "model_pred_train = logistic_regression.predict(X_train)\n",
    "model_pred_test = logistic_regression.predict(X_test)\n",
    "\n",
    "res_1 = cross_val_score(logistic_regression, X_train, y_train, scoring = 'accuracy', cv = 10)\n",
    "\n",
    "acc_logistic_regression, prec_logistic_regression, recall_logistic_regression, f1_logistic_regression = model_run(X_train, y_train, logistic_regression, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_sm = LogisticRegression(n_jobs=-1, random_state=15).fit(X_train_smote, y_train_smote)\n",
    "\n",
    "model_pred_train = logistic_regression_sm.predict(X_train_smote)\n",
    "model_pred_test = logistic_regression_sm.predict(X_test)\n",
    "\n",
    "res_2 = cross_val_score(logistic_regression_sm, X_train_smote, y_train_smote, scoring = 'accuracy', cv = 10)\n",
    "\n",
    "acc_logistic_regression_sm, prec_logistic_regression_sm, recall_logistic_regression_sm, f1_logistic_regression_sm = model_run(X_train_smote, y_train_smote, logistic_regression_sm, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - with SMOTE and hyper paramters tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "parameters = {\"C\":np.logspace(-3,3,7), 'penalty': ['l1', 'l2'], 'solver' : ['liblinear', 'sag', 'saga'], 'random_state':(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15)}\n",
    "\n",
    "\n",
    "grid_LR = GridSearchCV(estimator=LR, param_grid = parameters, cv = 2, n_jobs=-1)\n",
    "grid_LR.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid_LR.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",grid_LR.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid_LR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate variables based on list of best parameters\n",
    "print('Below are the variables to be used based on the gridsearch hyperparamter technique applied: ')\n",
    "C = list(grid_LR.best_params_.values())[0]\n",
    "print('   C = ', C)\n",
    "penalty = list(grid_LR.best_params_.values())[1]\n",
    "print('   penalty = ', penalty)\n",
    "random_state = list(grid_LR.best_params_.values())[2]\n",
    "print('   random_state = ', random_state)\n",
    "solver = list(grid_LR.best_params_.values())[3]\n",
    "print('   solver = ', solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_sm_hp = LogisticRegression(C=C, penalty=penalty, random_state=random_state, solver=solver, n_jobs=-1).fit(X_train_smote, y_train_smote)\n",
    "\n",
    "model_pred_train = logistic_regression_sm_hp.predict(X_train_smote)\n",
    "model_pred_test = logistic_regression_sm_hp.predict(X_test)\n",
    "\n",
    "res_3 = cross_val_score(logistic_regression_sm_hp, X_train_smote, y_train_smote, scoring = 'accuracy', cv = 10)\n",
    "\n",
    "acc_logistic_regression_sm_hp, prec_logistic_regression_sm_hp, recall_logistic_regression_sm_hp, f1_logistic_regression_sm_hp = model_run(X_train_smote, y_train_smote, logistic_regression_sm_hp, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTree - with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_sm = RandomForestClassifier().fit(X_train_smote, y_train_smote)\n",
    "\n",
    "model_pred_train = random_forest_sm.predict(X_train_smote)\n",
    "model_pred_test = random_forest_sm.predict(X_test)\n",
    "\n",
    "res_4 = cross_val_score(random_forest_sm, X_train_smote, y_train_smote, scoring = 'accuracy', cv = 10)\n",
    "\n",
    "acc_random_forest_sm, prec_random_forest_sm, recall_random_forest_sm, f1_random_forest_sm = model_run(X_train_smote, y_train_smote, random_forest_sm, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - with SMOTE and hyper parameters tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "rand_for = RandomForestClassifier().fit(X_train_smote, y_train_smote)\n",
    "\n",
    "parameters = {'n_estimators': [200, 300, 400, 500], 'max_features': ['auto', 'sqrt', 'log2'], \n",
    "               'max_depth': [4, 5, 6, 7, 8], 'criterion': ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=rand_for, param_grid = parameters, cv = 2, n_jobs=-1)\n",
    "grid.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\" Results from Grid Search \" )\n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",grid.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\",grid.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate variables based on list of best parameters\n",
    "print('Below are the variables to be used based on the gridsearch hyperparamter technique applied: ')\n",
    "criterion = list(grid.best_params_.values())[0]\n",
    "print('   criterion = ', criterion)\n",
    "max_depth = list(grid.best_params_.values())[1]\n",
    "print('   max_depth = ', max_depth)\n",
    "max_features = list(grid.best_params_.values())[2]\n",
    "print('   max_features = ', max_features)\n",
    "n_estimators = list(grid.best_params_.values())[3]\n",
    "print('   n_estimators = ', n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_sm_hp = RandomForestClassifier(criterion=criterion, max_depth=max_depth, max_features=max_features, n_estimators=n_estimators).fit(X_train_smote, y_train_smote)\n",
    "\n",
    "model_pred_train = random_forest_sm_hp.predict(X_train_smote)\n",
    "model_pred_test = random_forest_sm_hp.predict(X_test)\n",
    "\n",
    "res_5 = cross_val_score(random_forest_sm_hp, X_train_smote, y_train_smote, scoring = 'accuracy', cv = 10)\n",
    "\n",
    "acc_random_forest_sm_hp, prec_random_forest_sm_hp, recall_random_forest_sm_hp, f1_random_forest_sm_hp = model_run(X_train_smote, y_train_smote, random_forest_sm_hp, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list which contains classifiers \n",
    "classifiers = []\n",
    "classifiers.append(logistic_regression)\n",
    "classifiers.append(logistic_regression_sm)\n",
    "classifiers.append(logistic_regression_sm_hp)\n",
    "classifiers.append(random_forest_sm)\n",
    "classifiers.append(random_forest_sm_hp)\n",
    "print('Number of Classifiers: ',len(classifiers))\n",
    "\n",
    "# Number of Cross Validations\n",
    "cv = 10\n",
    "print('Number of Cross Validations: ', cv, '\\n','-'*40)\n",
    "\n",
    "# Create a list which contains cross validation results for each classifier\n",
    "cv_results = []\n",
    "cv_results.append(res_1)\n",
    "cv_results.append(res_2)\n",
    "cv_results.append(res_3)\n",
    "cv_results.append(res_4)\n",
    "cv_results.append(res_5)\n",
    "\n",
    "# for classifier in classifiers:\n",
    "#     cv_results.append(cross_val_score(classifier, X_train, y_train, scoring = 'accuracy', cv = 10))\n",
    "    \n",
    "# Mean and standard deviation of cross validation results for each classifier  \n",
    "cv_mean = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_mean.append(round(cv_result.mean()*100,2))\n",
    "    cv_std.append(round(cv_result.std(),3))\n",
    "\n",
    "algos = ['Logistic Regression - Initial Run',\n",
    "         'Logistic Regression - With SMOTE',\n",
    "         'Logistic Regression - With SMOTE and Hyperparamters Tuned',\n",
    "         'Random Forest - With SMOTE',\n",
    "         'Random Forest - With SMOTE and Hyperparamters Tuned'\n",
    "        ]\n",
    "\n",
    "acc_scores = [acc_logistic_regression,\n",
    "              acc_logistic_regression_sm,\n",
    "              acc_logistic_regression_sm_hp,\n",
    "              acc_random_forest_sm,\n",
    "              acc_random_forest_sm_hp\n",
    "             ]\n",
    "\n",
    "prec_scores = [prec_logistic_regression,\n",
    "              prec_logistic_regression_sm,\n",
    "              prec_logistic_regression_sm_hp,\n",
    "              prec_random_forest_sm,\n",
    "              prec_random_forest_sm_hp\n",
    "              ]\n",
    "\n",
    "recall_scores = [recall_logistic_regression,\n",
    "                 recall_logistic_regression_sm,\n",
    "                 recall_logistic_regression_sm_hp,\n",
    "                 recall_random_forest_sm,\n",
    "                 recall_random_forest_sm_hp\n",
    "                ]\n",
    "\n",
    "f1_scores = [f1_logistic_regression,\n",
    "             f1_logistic_regression_sm,\n",
    "             f1_logistic_regression_sm_hp,\n",
    "             f1_random_forest_sm,\n",
    "             f1_random_forest_sm_hp\n",
    "            ]\n",
    "\n",
    "    \n",
    "cv_res = pd.DataFrame({'Algorithm': algos,\n",
    "                       'Initial Accuracy Scores': acc_scores,\n",
    "                       'Cross Validation Mean': cv_mean, \n",
    "                       'Cross Validation Std': cv_std,\n",
    "                       'Precision Score': prec_scores,\n",
    "                       'Recall Scores': recall_scores,\n",
    "                       'F1 Scores': f1_scores\n",
    "                       })\n",
    "\n",
    "\n",
    "cv_res.sort_values(by = 'F1 Scores', ascending = False).set_index('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res1 = cv_res.drop(columns='Cross Validation Std').set_index('Algorithm').T\n",
    "sns.set(font_scale = 1.7)\n",
    "fig, ax = plt.subplots(figsize=(40,18))\n",
    "cv_res1.plot(kind='bar', ax=ax)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('Percent')\n",
    "plt.legend(loc='top right')\n",
    "show_values(ax)\n",
    "plt.ylim(0, 110)\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize F1 Scores to Identify the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('F1 Scores', \n",
    "            'Algorithm', \n",
    "            data = cv_res, \n",
    "            order = cv_res.sort_values(by = 'F1 Scores', \n",
    "                                       ascending = False)['Algorithm'], \n",
    "            palette = 'Set3', \n",
    "            **{'xerr': cv_std})\n",
    "\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('F1 Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = logistic_regression_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = selected_model.predict_proba(X_test)\n",
    "# Calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = roc_curve(y_test, probs[:,1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Prediction Results to Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = selected_model.predict_proba(x)[:, 1]\n",
    "# Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('gvkey', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of companies in this prediction is: ' ,len(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'index': df['index'], 'suit_pred': Y_pred})\n",
    "submit = submit[['suit_pred']]\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(submit)\n",
    "df.drop(columns='index', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['suitflag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x, base=5):\n",
    "    return int(base * round(float(x)/base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prediction_Score'] = df['suit_pred']*100\n",
    "df['Prediction_Score'] = df['Prediction_Score'].apply(lambda x: custom_round(x, base=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Prediction_Score'] = round(df['suit_pred']*100, 2)\n",
    "pd.value_counts(df['Prediction_Score'], dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist('suit_pred', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (25,10))\n",
    "# sns.set(font_scale = 2.5)\n",
    "\n",
    "\n",
    "# var = df['Prediction_Score']\n",
    "# ax = sns.countplot(x = var,\n",
    "#                    data = df, )\n",
    "\n",
    "# show_values(ax)\n",
    "# plt.ylim(0, 150)\n",
    "\n",
    "# plt.ylabel('Company Count')\n",
    "# plt.xlabel('Prediction Scores \\n(in %)')\n",
    "# # plt.title('Company Count by Prediction Score')\n",
    "# plt.xticks(rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = cross_val_score(logistic_regression_sm, X_test, y_test, cv=cv, scoring='accuracy')\n",
    "# model  = pd.DataFrame({\"Features\": X_train_smote.columns,\"Coefficient\":res.params.values})\n",
    "# model[\"Odds_Ratio\"] = model[\"Coefficient\"].apply(lambda x: np.exp(x))\n",
    "# model[[\"Coefficient\",\"Odds_Ratio\"]] = model[[\"Coefficient\",\"Odds_Ratio\"]].apply(lambda x: round(x,2))\n",
    "# model[\"Perc_Impact\"] = model[\"Odds_Ratio\"].apply(lambda x: (x-1)*100)\n",
    "# model = model.loc[model['Features']!='const'].sort_values(by='Odds_Ratio', ascending=False)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x.index.isin(['3580'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_sm_hp.predict_proba(x[x.index.isin(['3580'])])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_sm.predict_proba(x[x.index.isin(['3580'])])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(x[x.index.isin(['3580'])])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(logistic_regression_sm.coef_[0],index=x.columns, columns=['Coef']).sort_values('Coef',ascending=False)\n",
    "coef[abs(coef['Coef'])>.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_keep = list(coef[coef['Coef']>0.00].index)\n",
    "X_train_smote2 = X_train_smote[col_to_keep]\n",
    "X_test2 = X_test[col_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(X_train_smote2, X_test2, y_train_smote, y_test, DecisionTreeClassifier, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Results WITH SMOTE\n",
    "logistic_regression2, acc_logistic_regression2, prec_logistic_regression2, recall_logistic_regression2, f1_logistic_regression2 = model_run(X_train_smote2, X_test2, y_train_smote, y_test, LogisticRegression, 'test')\n",
    "#  = model_run(X_train_smote, X_test, y_train_smote, y_test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run(X_train_smote2, X_test2, y_train_smote, y_test, RandomForestClassifier, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(model, feature_names):\n",
    "    \n",
    "    print(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_features(logistic_regression, x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_loc = '../01_data/02_modified/'\n",
    "# X_train.to_csv(folder_loc+'X_train.csv')\n",
    "# X_test.to_csv(folder_loc+'X_test.csv')\n",
    "# y_train.to_csv(folder_loc+'y_train.csv')\n",
    "# y_test.to_csv(folder_loc+'y_test.csv')\n",
    "# X_train_smote.to_csv(folder_loc+'X_train_smote.csv', index=False)\n",
    "# y_train_smote.to_csv(folder_loc+'y_train_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
